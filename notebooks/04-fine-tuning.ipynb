{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7230dea",
   "metadata": {},
   "source": [
    "# 04 - Fine-Tuning: Training with QLoRA\n",
    "\n",
    "**Goal**: Fine-tune a language model on our Ethereum transaction dataset using QLoRA.\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What QLoRA is and why it enables training on consumer GPUs\n",
    "- How to configure 4-bit quantization and LoRA adapters\n",
    "- How to load and prepare datasets for training\n",
    "- How to execute training with live monitoring\n",
    "- How to manage VRAM usage and checkpoints\n",
    "- Google Colab compatibility tips\n",
    "\n",
    "**Prerequisites**: Completed `03-dataset-preparation.ipynb`, have prepared datasets\n",
    "\n",
    "**Hardware Requirements**: \n",
    "- 12-16GB VRAM GPU (RTX 3060/4060, T4, or better)\n",
    "- Or Google Colab with T4 GPU (free tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09e928",
   "metadata": {},
   "source": [
    "## üöÄ Google Colab Setup (Optional)\n",
    "\n",
    "**If running on Google Colab**, uncomment and run this cell first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google Colab Setup\n",
    "# # Uncomment these lines if running on Colab\n",
    "# \n",
    "# # Check GPU availability\n",
    "# !nvidia-smi\n",
    "# \n",
    "# # Clone repository (if not already done)\n",
    "# # !git clone https://github.com/YOUR_USERNAME/eth-finetuning-cookbook.git\n",
    "# # %cd eth-finetuning-cookbook\n",
    "# \n",
    "# # Install dependencies\n",
    "# !pip install -q -U transformers datasets peft accelerate bitsandbytes torch torchvision torchaudio\n",
    "# \n",
    "# # Verify installation\n",
    "# import torch\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA version: {torch.version.cuda}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491a256",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51072cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from eth_finetuning.training.config import TrainingConfig\n",
    "from eth_finetuning.training.trainer import (\n",
    "    setup_model_and_tokenizer,\n",
    "    create_trainer,\n",
    "    format_prompts,\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dd220",
   "metadata": {},
   "source": [
    "## Understanding QLoRA\n",
    "\n",
    "### What is QLoRA?\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation)** is a technique that makes it possible to fine-tune large language models (7B+ parameters) on consumer GPUs.\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "A 7B parameter model in full precision (32-bit) requires:\n",
    "- **28GB VRAM** just to load the model\n",
    "- **Additional VRAM** for optimizer states, gradients, activations\n",
    "- **Total: ~50-60GB VRAM** üò±\n",
    "\n",
    "### The Solution: QLoRA = Quantization + LoRA\n",
    "\n",
    "**1. Quantization (4-bit)**\n",
    "- Reduces model weights from 32-bit ‚Üí 4-bit\n",
    "- **Reduces VRAM: 28GB ‚Üí ~7GB** üéâ\n",
    "- Minimal accuracy loss with NF4 (NormalFloat4)\n",
    "\n",
    "**2. LoRA (Low-Rank Adaptation)**\n",
    "- Freezes base model (no gradient updates)\n",
    "- Adds small trainable adapter layers\n",
    "- **Only ~1-2% of parameters are trainable**\n",
    "- Adapter weights: ~0.5GB\n",
    "\n",
    "**Result:**\n",
    "- Base model (frozen, 4-bit): ~7GB\n",
    "- LoRA adapters (trainable): ~0.5GB\n",
    "- Activations & gradients: ~3-4GB\n",
    "- **Total: ~11-12GB VRAM** ‚úÖ\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "‚úì Train on consumer GPUs (RTX 3060, 4060, etc.)\n",
    "\n",
    "‚úì Fast training (only updating 1-2% of parameters)\n",
    "\n",
    "‚úì Small adapter files (~100MB vs 14GB for full model)\n",
    "\n",
    "‚úì Minimal quality loss vs full fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87087941",
   "metadata": {},
   "source": [
    "## Checking GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n‚úì CUDA available\")\n",
    "    print(f\"  GPU Model:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Total VRAM:     {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"  CUDA Version:   {torch.version.cuda}\")\n",
    "    print(f\"  PyTorch Device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Check current VRAM usage\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nCurrent VRAM Usage:\")\n",
    "    print(f\"  Allocated:      {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved:       {reserved:.2f} GB\")\n",
    "    \n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if total_vram >= 12:\n",
    "        print(f\"\\n‚úì GPU has sufficient VRAM for QLoRA fine-tuning\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU has less than 12GB VRAM\")\n",
    "        print(f\"   Training may fail or require smaller batch sizes\")\n",
    "else:\n",
    "    print(\"\\n‚úó CUDA not available\")\n",
    "    print(\"   GPU is required for training\")\n",
    "    print(\"   Consider using Google Colab with T4 GPU (free tier)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc5ab1",
   "metadata": {},
   "source": [
    "## Loading Training Configuration\n",
    "\n",
    "Our training configuration is stored in `configs/training_config.yaml` and includes:\n",
    "- Base model selection\n",
    "- Quantization settings (4-bit)\n",
    "- LoRA hyperparameters\n",
    "- Training hyperparameters\n",
    "- Memory optimization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / \"configs\" / \"training_config.yaml\"\n",
    "config = TrainingConfig.from_yaml(str(config_path))\n",
    "\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBase Model: {config.model_name}\")\n",
    "print(f\"\\nQuantization Settings:\")\n",
    "print(f\"  4-bit:          {config.load_in_4bit}\")\n",
    "print(f\"  Compute dtype:  {config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"  Quant type:     {config.bnb_4bit_quant_type}\")\n",
    "print(f\"\\nLoRA Settings:\")\n",
    "print(f\"  Rank (r):       {config.lora_r}\")\n",
    "print(f\"  Alpha:          {config.lora_alpha}\")\n",
    "print(f\"  Dropout:        {config.lora_dropout}\")\n",
    "print(f\"  Target modules: {', '.join(config.target_modules)}\")\n",
    "print(f\"\\nTraining Settings:\")\n",
    "print(f\"  Learning rate:  {config.learning_rate}\")\n",
    "print(f\"  Batch size:     {config.per_device_train_batch_size}\")\n",
    "print(f\"  Grad accum:     {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Epochs:         {config.num_train_epochs}\")\n",
    "print(f\"  Max seq length: {config.max_seq_length}\")\n",
    "print(f\"\\nEffective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cd55f",
   "metadata": {},
   "source": [
    "### Understanding Key Hyperparameters\n",
    "\n",
    "**LoRA Rank (r=16)**\n",
    "- Controls adapter expressiveness\n",
    "- Higher rank = more parameters = better fit, but more VRAM\n",
    "- 8-16 is typical for 7B models\n",
    "\n",
    "**LoRA Alpha (32)**\n",
    "- Scaling factor for adapter weights\n",
    "- Typically 2x the rank\n",
    "- Affects learning rate sensitivity\n",
    "\n",
    "**Gradient Accumulation (16 steps)**\n",
    "- Simulates larger batch sizes\n",
    "- Effective batch size = 1 √ó 16 = 16\n",
    "- Trades time for memory\n",
    "\n",
    "**Learning Rate (2e-4)**\n",
    "- Higher than full fine-tuning (typically 1e-5)\n",
    "- LoRA adapters can handle higher rates\n",
    "- Speeds up convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f687d99",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "Let's load the prepared dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c39b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_dir = project_root / \"data\" / \"datasets\"\n",
    "\n",
    "# Check if datasets exist\n",
    "train_file = dataset_dir / \"train.jsonl\"\n",
    "val_file = dataset_dir / \"validation.jsonl\"\n",
    "test_file = dataset_dir / \"test.jsonl\"\n",
    "\n",
    "if not all([train_file.exists(), val_file.exists(), test_file.exists()]):\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset files not found!\")\n",
    "    print(f\"   Expected location: {dataset_dir}\")\n",
    "    print(\"   Please run notebook 03-dataset-preparation.ipynb first\")\n",
    "else:\n",
    "    # Load dataset with HuggingFace\n",
    "    dataset = load_dataset(\n",
    "        'json',\n",
    "        data_files={\n",
    "            'train': str(train_file),\n",
    "            'validation': str(val_file),\n",
    "            'test': str(test_file),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Dataset loaded successfully\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train:      {len(dataset['train']):4d} examples\")\n",
    "    print(f\"  Validation: {len(dataset['validation']):4d} examples\")\n",
    "    print(f\"  Test:       {len(dataset['test']):4d} examples\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample training example:\")\n",
    "    sample = dataset['train'][0]\n",
    "    print(f\"\\nInstruction: {sample['instruction'][:80]}...\")\n",
    "    print(f\"Input:       {sample['input'][:80]}...\")\n",
    "    print(f\"Output:      {sample['output'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015fe4a",
   "metadata": {},
   "source": [
    "## Setting Up Model and Tokenizer\n",
    "\n",
    "This is where QLoRA magic happens:\n",
    "1. Load model with 4-bit quantization\n",
    "2. Prepare model for k-bit training\n",
    "3. Add LoRA adapters\n",
    "4. Enable gradient checkpointing\n",
    "\n",
    "**Note**: Model download may take a few minutes on first run (~3-4GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING BASE MODEL WITH QUANTIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis may take a few minutes on first run...\")\n",
    "print(\"(Model will be cached for future use)\\n\")\n",
    "\n",
    "# Track initial VRAM\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    initial_vram = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"Initial VRAM: {initial_vram:.2f} GB\\n\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "start_time = time.time()\n",
    "model, tokenizer = setup_model_and_tokenizer(config)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Model and tokenizer loaded in {load_time:.1f} seconds\")\n",
    "\n",
    "# Check VRAM usage after loading\n",
    "if torch.cuda.is_available():\n",
    "    model_vram = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"\\nVRAM after loading:\")\n",
    "    print(f\"  Total allocated: {model_vram:.2f} GB\")\n",
    "    print(f\"  Model size:      ~{model_vram - initial_vram:.2f} GB\")\n",
    "    \n",
    "    if model_vram < 10:\n",
    "        print(f\"\\n‚úì Model fits comfortably in VRAM (< 10GB)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Model using {model_vram:.2f} GB\")\n",
    "\n",
    "# Show trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_pct = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters:     {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable percentage: {trainable_pct:.2f}%\")\n",
    "print(f\"\\n‚úì Only {trainable_pct:.2f}% of parameters will be updated (QLoRA efficiency!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67003e",
   "metadata": {},
   "source": [
    "## Preparing Dataset for Training\n",
    "\n",
    "We need to:\n",
    "1. Format examples as prompts\n",
    "2. Tokenize with proper padding/truncation\n",
    "3. Add labels for causal language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10469e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PREPARING DATASET FOR TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Format and tokenize datasets\n",
    "print(\"\\nFormatting and tokenizing...\")\n",
    "\n",
    "# Use our dataset formatting function\n",
    "train_dataset = format_prompts(dataset['train'], tokenizer, config)\n",
    "eval_dataset = format_prompts(dataset['validation'], tokenizer, config)\n",
    "\n",
    "print(f\"\\n‚úì Datasets prepared\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Eval:  {len(eval_dataset)} examples\")\n",
    "\n",
    "# Show tokenization statistics\n",
    "sample_lengths = [len(train_dataset[i]['input_ids']) for i in range(min(100, len(train_dataset)))]\n",
    "print(f\"\\nToken length statistics (first 100 examples):\")\n",
    "print(f\"  Mean:   {sum(sample_lengths) / len(sample_lengths):.0f} tokens\")\n",
    "print(f\"  Min:    {min(sample_lengths)} tokens\")\n",
    "print(f\"  Max:    {max(sample_lengths)} tokens\")\n",
    "print(f\"  Limit:  {config.max_seq_length} tokens\")\n",
    "\n",
    "if max(sample_lengths) <= config.max_seq_length:\n",
    "    print(f\"\\n‚úì All samples fit within max sequence length\")\n",
    "else:\n",
    "    truncated = sum(1 for l in sample_lengths if l > config.max_seq_length)\n",
    "    print(f\"\\n‚ö†Ô∏è  {truncated} sample(s) will be truncated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df301a2",
   "metadata": {},
   "source": [
    "## Creating Trainer\n",
    "\n",
    "The HuggingFace `Trainer` handles:\n",
    "- Training loop\n",
    "- Gradient accumulation\n",
    "- Mixed precision (FP16/BF16)\n",
    "- Checkpointing\n",
    "- Evaluation\n",
    "- Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CREATING TRAINER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / \"models\" / \"fine-tuned\" / \"eth-intent-notebook\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create trainer\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    config=config,\n",
    "    output_dir=str(output_dir),\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Trainer created successfully\")\n",
    "print(f\"\\nTraining arguments:\")\n",
    "print(f\"  Output directory:  {output_dir}\")\n",
    "print(f\"  Logging steps:     {config.logging_steps}\")\n",
    "print(f\"  Eval steps:        {config.eval_steps}\")\n",
    "print(f\"  Save steps:        {config.save_steps}\")\n",
    "print(f\"  Gradient checkpt:  {config.gradient_checkpointing}\")\n",
    "print(f\"  FP16:              {config.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7bf75",
   "metadata": {},
   "source": [
    "## Training Configuration Summary\n",
    "\n",
    "Let's review what will happen during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training statistics\n",
    "num_examples = len(train_dataset)\n",
    "batch_size = config.per_device_train_batch_size\n",
    "grad_accum = config.gradient_accumulation_steps\n",
    "effective_batch = batch_size * grad_accum\n",
    "steps_per_epoch = num_examples // effective_batch\n",
    "total_steps = steps_per_epoch * config.num_train_epochs\n",
    "\n",
    "print(\"TRAINING PLAN\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training examples:     {num_examples}\")\n",
    "print(f\"  Evaluation examples:   {len(eval_dataset)}\")\n",
    "print(f\"\\nBatch Configuration:\")\n",
    "print(f\"  Per-device batch size: {batch_size}\")\n",
    "print(f\"  Gradient accumulation: {grad_accum}\")\n",
    "print(f\"  Effective batch size:  {effective_batch}\")\n",
    "print(f\"\\nTraining Schedule:\")\n",
    "print(f\"  Epochs:                {config.num_train_epochs}\")\n",
    "print(f\"  Steps per epoch:       ~{steps_per_epoch}\")\n",
    "print(f\"  Total training steps:  ~{total_steps}\")\n",
    "print(f\"  Warmup steps:          {config.warmup_steps}\")\n",
    "print(f\"\\nCheckpointing:\")\n",
    "print(f\"  Save every:            {config.save_steps} steps\")\n",
    "print(f\"  Eval every:            {config.eval_steps} steps\")\n",
    "print(f\"  Log every:             {config.logging_steps} steps\")\n",
    "\n",
    "# Estimate time (very rough)\n",
    "if num_examples > 0:\n",
    "    # Rough estimate: 1-2 seconds per step on T4\n",
    "    estimated_time_min = total_steps * 1.0 / 60  # optimistic\n",
    "    estimated_time_max = total_steps * 2.0 / 60  # conservative\n",
    "    print(f\"\\nEstimated Training Time:\")\n",
    "    print(f\"  Optimistic:  {estimated_time_min:.0f} minutes ({estimated_time_min/60:.1f} hours)\")\n",
    "    print(f\"  Conservative: {estimated_time_max:.0f} minutes ({estimated_time_max/60:.1f} hours)\")\n",
    "    print(f\"\\n  (Actual time depends on GPU, data complexity, and system load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725d307",
   "metadata": {},
   "source": [
    "## üöÄ Starting Training\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "- Training will take **significant time** (potentially hours)\n",
    "- You can monitor progress with the progress bar\n",
    "- Loss should decrease over time\n",
    "- **You can interrupt training** with Kernel ‚Üí Interrupt\n",
    "- Training will save checkpoints every `save_steps`\n",
    "- You can resume from checkpoint later\n",
    "\n",
    "**For production training**, use the CLI script instead:\n",
    "```bash\n",
    "python scripts/training/train_model.py \\\n",
    "    --dataset data/datasets \\\n",
    "    --output models/fine-tuned/eth-intent-v1 \\\n",
    "    --config configs/training_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è  This will take significant time!\")\n",
    "print(\"   - Monitor VRAM usage with nvidia-smi\")\n",
    "print(\"   - Watch for decreasing loss\")\n",
    "print(\"   - You can interrupt and resume from checkpoint\\n\")\n",
    "\n",
    "# Check VRAM before training\n",
    "if torch.cuda.is_available():\n",
    "    pre_train_vram = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"VRAM before training: {pre_train_vram:.2f} GB\\n\")\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úì TRAINING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTraining time: {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
    "    print(f\"\\nFinal metrics:\")\n",
    "    print(f\"  Train loss:    {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Steps:         {train_result.global_step}\")\n",
    "    \n",
    "    # Check final VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        final_vram = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        peak_vram = torch.cuda.max_memory_allocated(0) / 1024**3\n",
    "        print(f\"\\nVRAM usage:\")\n",
    "        print(f\"  Current:       {final_vram:.2f} GB\")\n",
    "        print(f\"  Peak:          {peak_vram:.2f} GB\")\n",
    "        \n",
    "        if peak_vram < 12:\n",
    "            print(f\"\\n‚úì Training completed within 12GB VRAM constraint\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "    print(f\"   Training time so far: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"   Latest checkpoint saved to: {output_dir}\")\n",
    "    print(f\"   You can resume training by re-running this cell\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Training failed with error: {e}\")\n",
    "    print(f\"   Check VRAM usage and configuration\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a37184",
   "metadata": {},
   "source": [
    "## Saving the Fine-Tuned Model\n",
    "\n",
    "Let's save the trained adapter and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAVING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save the adapter\n",
    "final_output_dir = output_dir / \"final\"\n",
    "trainer.model.save_pretrained(str(final_output_dir))\n",
    "tokenizer.save_pretrained(str(final_output_dir))\n",
    "\n",
    "print(f\"\\n‚úì Model saved to: {final_output_dir}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "for file in final_output_dir.iterdir():\n",
    "    file_size = file.stat().st_size / 1024**2  # MB\n",
    "    print(f\"  {file.name:30s} ({file_size:6.1f} MB)\")\n",
    "\n",
    "print(f\"\\n‚úì Fine-tuning complete!\")\n",
    "print(f\"\\nAdapter files are small (~100MB) compared to full model (~14GB)\")\n",
    "print(f\"This is the power of LoRA - only the adapter weights are saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91184d1e",
   "metadata": {},
   "source": [
    "## Visualizing Training Metrics\n",
    "\n",
    "Let's load and visualize the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78821e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training loss\n",
    "train_logs = [log for log in log_history if 'loss' in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "if train_logs:\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    steps = [log['step'] for log in train_logs]\n",
    "    losses = [log['loss'] for log in train_logs]\n",
    "    \n",
    "    axes[0].plot(steps, losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot 2: Training vs Evaluation Loss\n",
    "    if eval_logs:\n",
    "        eval_steps = [log['step'] for log in eval_logs]\n",
    "        eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "        \n",
    "        axes[1].plot(steps, losses, 'b-', linewidth=2, label='Training Loss', alpha=0.7)\n",
    "        axes[1].plot(eval_steps, eval_losses, 'r-', linewidth=2, marker='o', label='Eval Loss')\n",
    "        axes[1].set_title('Training vs Evaluation Loss', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No evaluation logs available', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "        axes[1].set_title('Evaluation Loss', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nTraining Statistics:\")\n",
    "    print(f\"  Initial loss:  {losses[0]:.4f}\")\n",
    "    print(f\"  Final loss:    {losses[-1]:.4f}\")\n",
    "    print(f\"  Improvement:   {(losses[0] - losses[-1])/losses[0]*100:.1f}%\")\n",
    "    print(f\"  Total steps:   {steps[-1]}\")\n",
    "    \n",
    "    if eval_logs:\n",
    "        print(f\"\\nEvaluation Statistics:\")\n",
    "        print(f\"  Final eval loss: {eval_losses[-1]:.4f}\")\n",
    "        print(f\"  Evaluations:     {len(eval_logs)}\")\n",
    "else:\n",
    "    print(\"No training logs available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614a12f",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úì **QLoRA enables fine-tuning 7B models on consumer GPUs** (12GB VRAM)\n",
    "\n",
    "‚úì **4-bit quantization** reduces model size from 28GB ‚Üí 7GB\n",
    "\n",
    "‚úì **LoRA adapters** add minimal parameters (~1-2%) while maintaining quality\n",
    "\n",
    "‚úì **Gradient accumulation** simulates larger batch sizes within VRAM constraints\n",
    "\n",
    "‚úì **Checkpointing** allows training to be interrupted and resumed\n",
    "\n",
    "‚úì **Adapter-only saving** means small file sizes (~100MB vs 14GB)\n",
    "\n",
    "## Troubleshooting Tips\n",
    "\n",
    "**Issue**: CUDA out of memory\n",
    "- **Solution**: Reduce `per_device_train_batch_size` to 1, or reduce `max_seq_length`\n",
    "\n",
    "**Issue**: Training is very slow\n",
    "- **Solution**: Check if gradient accumulation is set correctly, verify GPU is being used\n",
    "\n",
    "**Issue**: Loss not decreasing\n",
    "- **Solution**: Check learning rate, verify dataset quality, increase training steps\n",
    "\n",
    "**Issue**: Evaluation loss increasing\n",
    "- **Solution**: Overfitting - reduce epochs, increase LoRA dropout, or get more data\n",
    "\n",
    "**Issue**: Model download fails\n",
    "- **Solution**: Check internet connection, HuggingFace API status, or use cached model\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**05-evaluation.ipynb**), we'll learn how to:\n",
    "- Load the fine-tuned model for inference\n",
    "- Run evaluation on the test set\n",
    "- Calculate accuracy metrics\n",
    "- Visualize results with confusion matrices\n",
    "- Analyze model predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** ‚Üí `notebooks/05-evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
