{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31215ad9",
   "metadata": {},
   "source": [
    "# 05 - Evaluation: Testing Model Performance\n",
    "\n",
    "**Goal**: Evaluate the fine-tuned model's performance on the test set.\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How to load fine-tuned models with adapters\n",
    "- How to run batch inference on test data\n",
    "- How to calculate accuracy metrics (amounts, addresses, protocols)\n",
    "- How to create confusion matrices\n",
    "- How to analyze model predictions\n",
    "- How to visualize performance breakdowns\n",
    "\n",
    "**Prerequisites**: Completed `04-fine-tuning.ipynb`, have trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60766e8c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from eth_finetuning.evaluation.evaluator import (\n",
    "    load_model_for_evaluation,\n",
    "    run_inference,\n",
    "    parse_json_output,\n",
    ")\n",
    "from eth_finetuning.evaluation.metrics import (\n",
    "    calculate_accuracy_metrics,\n",
    "    calculate_per_protocol_metrics,\n",
    "    calculate_readability_score,\n",
    ")\n",
    "from eth_finetuning.evaluation.report import generate_evaluation_report\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92ae2f",
   "metadata": {},
   "source": [
    "## Loading Test Dataset\n",
    "\n",
    "Let's load the test set that was held out during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING TEST DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load test dataset\n",
    "test_file = project_root / \"data\" / \"datasets\" / \"test.jsonl\"\n",
    "\n",
    "if not test_file.exists():\n",
    "    print(f\"\\n⚠️  Test file not found: {test_file}\")\n",
    "    print(\"   Please run notebook 03-dataset-preparation.ipynb first\")\n",
    "else:\n",
    "    test_dataset = load_dataset('json', data_files=str(test_file))['train']\n",
    "    \n",
    "    print(f\"\\n✓ Loaded test dataset\")\n",
    "    print(f\"  Total examples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Analyze test set composition\n",
    "    protocols = []\n",
    "    for example in test_dataset:\n",
    "        try:\n",
    "            intent = json.loads(example['output'])\n",
    "            protocols.append(intent.get('protocol', 'unknown'))\n",
    "        except:\n",
    "            protocols.append('unknown')\n",
    "    \n",
    "    protocol_counts = Counter(protocols)\n",
    "    print(f\"\\nTest set composition:\")\n",
    "    for protocol, count in protocol_counts.most_common():\n",
    "        pct = count / len(test_dataset) * 100\n",
    "        print(f\"  {protocol:15s}: {count:3d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65344f44",
   "metadata": {},
   "source": [
    "## Loading Fine-Tuned Model\n",
    "\n",
    "Now let's load the model we trained in the previous notebook.\n",
    "\n",
    "**Note**: This loads the model with the adapter merged, ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define model path\n",
    "model_dir = project_root / \"models\" / \"fine-tuned\" / \"eth-intent-notebook\" / \"final\"\n",
    "\n",
    "# Check if model exists\n",
    "if not model_dir.exists():\n",
    "    print(f\"\\n⚠️  Model not found at: {model_dir}\")\n",
    "    print(\"   Please train a model first using notebook 04-fine-tuning.ipynb\")\n",
    "    print(\"\\n   Alternative: Use a different model path\")\n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        project_root / \"models\" / \"fine-tuned\" / \"eth-intent-extractor-v1\",\n",
    "        project_root / \"models\" / \"fine-tuned\" / \"checkpoint-latest\",\n",
    "    ]\n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            print(f\"   Found alternative: {alt_path}\")\n",
    "            model_dir = alt_path\n",
    "            break\n",
    "\n",
    "if model_dir.exists():\n",
    "    print(f\"\\nLoading model from: {model_dir}\")\n",
    "    print(\"This may take a few minutes...\\n\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_for_evaluation(str(model_dir))\n",
    "    \n",
    "    print(\"\\n✓ Model loaded successfully\")\n",
    "    print(f\"  Model type: {type(model).__name__}\")\n",
    "    print(f\"  Tokenizer:  {type(tokenizer).__name__}\")\n",
    "    \n",
    "    # Check VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        vram = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        print(f\"  VRAM usage: {vram:.2f} GB\")\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    print(\"\\n✓ Model set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89b6fa",
   "metadata": {},
   "source": [
    "## Running Batch Inference\n",
    "\n",
    "Let's run inference on all test examples and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUNNING BATCH INFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProcessing {len(test_dataset)} test examples...\")\n",
    "print(\"This may take several minutes\\n\")\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "raw_outputs = []\n",
    "\n",
    "# Process each test example\n",
    "for i, example in enumerate(test_dataset):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Progress: {i}/{len(test_dataset)} ({i/len(test_dataset)*100:.0f}%)\")\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = f\"{example['instruction']}\\n\\nInput: {example['input']}\\n\\nOutput:\"\n",
    "    \n",
    "    # Run inference\n",
    "    output = run_inference(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,  # Low temperature for deterministic output\n",
    "    )\n",
    "    \n",
    "    raw_outputs.append(output)\n",
    "    \n",
    "    # Parse prediction\n",
    "    pred_intent = parse_json_output(output)\n",
    "    predictions.append(pred_intent)\n",
    "    \n",
    "    # Parse ground truth\n",
    "    try:\n",
    "        truth_intent = json.loads(example['output'])\n",
    "        ground_truths.append(truth_intent)\n",
    "    except json.JSONDecodeError:\n",
    "        ground_truths.append(None)\n",
    "\n",
    "print(f\"\\n✓ Inference complete\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")\n",
    "print(f\"  Valid predictions: {sum(1 for p in predictions if p is not None)}\")\n",
    "print(f\"  Failed to parse:   {sum(1 for p in predictions if p is None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcb746",
   "metadata": {},
   "source": [
    "## Analyzing Predictions\n",
    "\n",
    "Let's examine some predictions to see how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first 3 predictions\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\nGround Truth:\")\n",
    "    if ground_truths[i]:\n",
    "        print(json.dumps(ground_truths[i], indent=2))\n",
    "    else:\n",
    "        print(\"  (Failed to parse)\")\n",
    "    \n",
    "    print(\"\\nPrediction:\")\n",
    "    if predictions[i]:\n",
    "        print(json.dumps(predictions[i], indent=2))\n",
    "    else:\n",
    "        print(\"  (Failed to parse)\")\n",
    "        print(f\"\\n  Raw output: {raw_outputs[i][:200]}...\")\n",
    "    \n",
    "    # Compare key fields\n",
    "    if predictions[i] and ground_truths[i]:\n",
    "        print(\"\\nComparison:\")\n",
    "        for key in ['action', 'protocol', 'outcome']:\n",
    "            pred_val = predictions[i].get(key, 'N/A')\n",
    "            truth_val = ground_truths[i].get(key, 'N/A')\n",
    "            match = '✓' if pred_val == truth_val else '✗'\n",
    "            print(f\"  {match} {key:10s}: {pred_val:15s} vs {truth_val:15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec005196",
   "metadata": {},
   "source": [
    "## Calculating Accuracy Metrics\n",
    "\n",
    "Now let's calculate comprehensive accuracy metrics:\n",
    "- **Overall accuracy**: Percentage of perfectly matched predictions\n",
    "- **Amount accuracy**: Accuracy of numerical amounts (±1% tolerance)\n",
    "- **Address accuracy**: Accuracy of Ethereum addresses\n",
    "- **Protocol accuracy**: Protocol classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CALCULATING ACCURACY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_accuracy_metrics(\n",
    "    predictions=predictions,\n",
    "    ground_truths=ground_truths,\n",
    ")\n",
    "\n",
    "print(\"\\nOVERALL METRICS\")\n",
    "print(f\"  Overall Accuracy:  {metrics['overall_accuracy']*100:6.2f}%\")\n",
    "print(f\"  Amount Accuracy:   {metrics['amount_accuracy']*100:6.2f}%\")\n",
    "print(f\"  Address Accuracy:  {metrics['address_accuracy']*100:6.2f}%\")\n",
    "print(f\"  Protocol Accuracy: {metrics['protocol_accuracy']*100:6.2f}%\")\n",
    "\n",
    "# Check against targets\n",
    "target_accuracy = 0.90  # 90% target from SPEC\n",
    "print(f\"\\nTarget: {target_accuracy*100:.0f}% accuracy\")\n",
    "\n",
    "all_above_target = all([\n",
    "    metrics['amount_accuracy'] >= target_accuracy,\n",
    "    metrics['address_accuracy'] >= target_accuracy,\n",
    "    metrics['protocol_accuracy'] >= target_accuracy,\n",
    "])\n",
    "\n",
    "if all_above_target:\n",
    "    print(\"\\n✓ All metrics meet 90% accuracy target!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Some metrics below 90% target:\")\n",
    "    if metrics['amount_accuracy'] < target_accuracy:\n",
    "        print(f\"   - Amount accuracy: {metrics['amount_accuracy']*100:.2f}%\")\n",
    "    if metrics['address_accuracy'] < target_accuracy:\n",
    "        print(f\"   - Address accuracy: {metrics['address_accuracy']*100:.2f}%\")\n",
    "    if metrics['protocol_accuracy'] < target_accuracy:\n",
    "        print(f\"   - Protocol accuracy: {metrics['protocol_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f2abf",
   "metadata": {},
   "source": [
    "## Per-Protocol Performance\n",
    "\n",
    "Let's break down performance by protocol to identify strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PER-PROTOCOL PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate per-protocol metrics\n",
    "protocol_metrics = calculate_per_protocol_metrics(\n",
    "    predictions=predictions,\n",
    "    ground_truths=ground_truths,\n",
    ")\n",
    "\n",
    "# Display as table\n",
    "protocol_data = []\n",
    "for protocol, stats in protocol_metrics.items():\n",
    "    protocol_data.append({\n",
    "        'Protocol': protocol,\n",
    "        'Count': stats['count'],\n",
    "        'Accuracy': f\"{stats['accuracy']*100:.1f}%\",\n",
    "        'Amount Acc': f\"{stats.get('amount_accuracy', 0)*100:.1f}%\",\n",
    "        'Address Acc': f\"{stats.get('address_accuracy', 0)*100:.1f}%\",\n",
    "    })\n",
    "\n",
    "df_protocols = pd.DataFrame(protocol_data)\n",
    "print(\"\\n\" + df_protocols.to_string(index=False))\n",
    "\n",
    "# Identify best and worst performing protocols\n",
    "accuracies = [(p, s['accuracy']) for p, s in protocol_metrics.items()]\n",
    "accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nBest performing:  {accuracies[0][0]} ({accuracies[0][1]*100:.1f}%)\")\n",
    "print(f\"Worst performing: {accuracies[-1][0]} ({accuracies[-1][1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce7608",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Let's visualize protocol classification with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29297696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract protocols for confusion matrix\n",
    "pred_protocols = [p.get('protocol', 'unknown') if p else 'unknown' for p in predictions]\n",
    "truth_protocols = [t.get('protocol', 'unknown') if t else 'unknown' for t in ground_truths]\n",
    "\n",
    "# Get unique protocols\n",
    "all_protocols = sorted(set(pred_protocols + truth_protocols))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(truth_protocols, pred_protocols, labels=all_protocols)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=all_protocols,\n",
    "    yticklabels=all_protocols,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Protocol Classification Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Protocol', fontsize=12)\n",
    "plt.xlabel('Predicted Protocol', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDiagonal elements = correct predictions\")\n",
    "print(\"Off-diagonal elements = misclassifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ebe08",
   "metadata": {},
   "source": [
    "## Visualizing Metric Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b80a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Overall Metrics Bar Chart\n",
    "metric_names = ['Overall', 'Amount', 'Address', 'Protocol']\n",
    "metric_values = [\n",
    "    metrics['overall_accuracy'],\n",
    "    metrics['amount_accuracy'],\n",
    "    metrics['address_accuracy'],\n",
    "    metrics['protocol_accuracy'],\n",
    "]\n",
    "colors = ['steelblue', 'coral', 'lightgreen', 'mediumpurple']\n",
    "\n",
    "bars = axes[0, 0].bar(metric_names, metric_values, color=colors)\n",
    "axes[0, 0].axhline(y=0.90, color='red', linestyle='--', label='90% Target')\n",
    "axes[0, 0].set_title('Accuracy Metrics Overview', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim(0, 1.0)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{value*100:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Per-Protocol Accuracy\n",
    "protocols_sorted = sorted(protocol_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "proto_names = [p[0] for p in protocols_sorted]\n",
    "proto_accs = [p[1]['accuracy'] for p in protocols_sorted]\n",
    "\n",
    "axes[0, 1].barh(proto_names, proto_accs, color='steelblue')\n",
    "axes[0, 1].axvline(x=0.90, color='red', linestyle='--', label='90% Target')\n",
    "axes[0, 1].set_title('Per-Protocol Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Accuracy')\n",
    "axes[0, 1].set_xlim(0, 1.0)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Sample Count by Protocol\n",
    "proto_counts = [p[1]['count'] for p in protocols_sorted]\n",
    "axes[1, 0].barh(proto_names, proto_counts, color='coral')\n",
    "axes[1, 0].set_title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Examples')\n",
    "\n",
    "# Plot 4: Metric Comparison\n",
    "metric_comparison = pd.DataFrame({\n",
    "    'Metric': ['Amount', 'Address', 'Protocol'],\n",
    "    'Accuracy': [\n",
    "        metrics['amount_accuracy'],\n",
    "        metrics['address_accuracy'],\n",
    "        metrics['protocol_accuracy'],\n",
    "    ],\n",
    "    'Target': [0.90, 0.90, 0.90]\n",
    "})\n",
    "\n",
    "x = np.arange(len(metric_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, metric_comparison['Accuracy'], width, label='Actual', color='steelblue')\n",
    "axes[1, 1].bar(x + width/2, metric_comparison['Target'], width, label='Target', color='red', alpha=0.5)\n",
    "axes[1, 1].set_title('Target vs Actual Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metric_comparison['Metric'])\n",
    "axes[1, 1].set_ylim(0, 1.0)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c37d1",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze prediction errors to understand failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d650e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Categorize errors\n",
    "error_types = defaultdict(list)\n",
    "\n",
    "for i, (pred, truth) in enumerate(zip(predictions, ground_truths)):\n",
    "    if pred is None:\n",
    "        error_types['parsing_failed'].append(i)\n",
    "        continue\n",
    "    \n",
    "    if truth is None:\n",
    "        continue\n",
    "    \n",
    "    # Check for specific errors\n",
    "    if pred.get('protocol') != truth.get('protocol'):\n",
    "        error_types['protocol_mismatch'].append(i)\n",
    "    \n",
    "    if pred.get('action') != truth.get('action'):\n",
    "        error_types['action_mismatch'].append(i)\n",
    "    \n",
    "    if pred.get('outcome') != truth.get('outcome'):\n",
    "        error_types['outcome_mismatch'].append(i)\n",
    "    \n",
    "    # Check amount accuracy\n",
    "    pred_amounts = pred.get('amounts', [])\n",
    "    truth_amounts = truth.get('amounts', [])\n",
    "    if pred_amounts != truth_amounts:\n",
    "        error_types['amount_mismatch'].append(i)\n",
    "\n",
    "# Display error summary\n",
    "print(\"\\nError Type Summary:\")\n",
    "for error_type, indices in sorted(error_types.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    count = len(indices)\n",
    "    pct = count / len(predictions) * 100\n",
    "    print(f\"  {error_type:20s}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Show example errors\n",
    "if error_types['protocol_mismatch']:\n",
    "    print(\"\\nExample Protocol Mismatch:\")\n",
    "    idx = error_types['protocol_mismatch'][0]\n",
    "    print(f\"  Truth:      {ground_truths[idx].get('protocol')}\")\n",
    "    print(f\"  Prediction: {predictions[idx].get('protocol')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ee587",
   "metadata": {},
   "source": [
    "## Generating Evaluation Report\n",
    "\n",
    "Let's generate a comprehensive markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GENERATING EVALUATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / \"outputs\" / \"reports\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_path = output_dir / \"evaluation_report.md\"\n",
    "\n",
    "# Generate report\n",
    "report = generate_evaluation_report(\n",
    "    metrics=metrics,\n",
    "    protocol_metrics=protocol_metrics,\n",
    "    test_size=len(test_dataset),\n",
    ")\n",
    "\n",
    "# Save report\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n✓ Report saved to: {report_path}\")\n",
    "\n",
    "# Display report preview\n",
    "print(\"\\nReport Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(report[:1000])\n",
    "if len(report) > 1000:\n",
    "    print(\"\\n... (truncated)\")\n",
    "    print(f\"\\nFull report: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fa165",
   "metadata": {},
   "source": [
    "## Saving Predictions\n",
    "\n",
    "Save predictions for future analysis or debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAVING PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create predictions output\n",
    "predictions_output = []\n",
    "for i, (pred, truth, raw) in enumerate(zip(predictions, ground_truths, raw_outputs)):\n",
    "    predictions_output.append({\n",
    "        'example_id': i,\n",
    "        'ground_truth': truth,\n",
    "        'prediction': pred,\n",
    "        'raw_output': raw,\n",
    "        'correct': pred == truth if pred and truth else False,\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "predictions_dir = project_root / \"outputs\" / \"predictions\"\n",
    "predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "predictions_file = predictions_dir / \"test_predictions.json\"\n",
    "\n",
    "with open(predictions_file, 'w') as f:\n",
    "    json.dump(predictions_output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Predictions saved to: {predictions_file}\")\n",
    "print(f\"  Total predictions: {len(predictions_output)}\")\n",
    "\n",
    "# Also save metrics\n",
    "metrics_file = predictions_dir.parent / \"metrics\" / \"test_metrics.json\"\n",
    "metrics_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_output = {\n",
    "    'overall_metrics': metrics,\n",
    "    'protocol_metrics': protocol_metrics,\n",
    "    'test_size': len(test_dataset),\n",
    "}\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics_output, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metrics saved to: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d10cdc",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Test Set: {len(test_dataset)} examples\")\n",
    "print(f\"\\n🎯 Overall Performance:\")\n",
    "print(f\"   Overall Accuracy:  {metrics['overall_accuracy']*100:6.2f}%\")\n",
    "print(f\"   Amount Accuracy:   {metrics['amount_accuracy']*100:6.2f}%\")\n",
    "print(f\"   Address Accuracy:  {metrics['address_accuracy']*100:6.2f}%\")\n",
    "print(f\"   Protocol Accuracy: {metrics['protocol_accuracy']*100:6.2f}%\")\n",
    "\n",
    "print(f\"\\n🏆 Best Protocol: {accuracies[0][0]} ({accuracies[0][1]*100:.1f}%)\")\n",
    "print(f\"📉 Worst Protocol: {accuracies[-1][0]} ({accuracies[-1][1]*100:.1f}%)\")\n",
    "\n",
    "# Target achievement\n",
    "target_met = sum([\n",
    "    metrics['amount_accuracy'] >= 0.90,\n",
    "    metrics['address_accuracy'] >= 0.90,\n",
    "    metrics['protocol_accuracy'] >= 0.90,\n",
    "])\n",
    "\n",
    "print(f\"\\n✓ Target Achievement: {target_met}/3 metrics above 90%\")\n",
    "\n",
    "if target_met == 3:\n",
    "    print(\"\\n🎉 SUCCESS: All accuracy targets met!\")\n",
    "elif target_met >= 2:\n",
    "    print(\"\\n👍 GOOD: Most accuracy targets met\")\n",
    "else:\n",
    "    print(\"\\n⚠️  NEEDS IMPROVEMENT: Consider more training or data\")\n",
    "\n",
    "print(f\"\\n📁 Output Files:\")\n",
    "print(f\"   Report:      {report_path}\")\n",
    "print(f\"   Predictions: {predictions_file}\")\n",
    "print(f\"   Metrics:     {metrics_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Evaluation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a77fb3",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✓ **Model Loading**: Fine-tuned adapters load quickly (~100MB vs 14GB full model)\n",
    "\n",
    "✓ **Batch Inference**: Process test sets efficiently with proper prompt formatting\n",
    "\n",
    "✓ **Comprehensive Metrics**: Multiple accuracy measures reveal different aspects of performance\n",
    "\n",
    "✓ **Per-Protocol Analysis**: Identifies which transaction types model handles best\n",
    "\n",
    "✓ **Confusion Matrix**: Visualizes classification errors and patterns\n",
    "\n",
    "✓ **Error Analysis**: Categorizes failures to guide improvement efforts\n",
    "\n",
    "## Improvement Strategies\n",
    "\n",
    "**If accuracy is below target:**\n",
    "\n",
    "1. **More Training Data**: Collect additional examples, especially for underperforming protocols\n",
    "2. **Longer Training**: Increase epochs or training steps\n",
    "3. **Hyperparameter Tuning**: Adjust learning rate, LoRA rank, batch size\n",
    "4. **Data Quality**: Review and improve dataset quality, fix labeling errors\n",
    "5. **Prompt Engineering**: Refine instruction templates for clarity\n",
    "6. **Model Selection**: Try different base models (e.g., Llama-2 vs Mistral)\n",
    "\n",
    "**If specific protocols underperform:**\n",
    "\n",
    "1. **Stratified Sampling**: Ensure balanced representation in training\n",
    "2. **Protocol-Specific Fine-tuning**: Create focused datasets for problematic protocols\n",
    "3. **Data Augmentation**: Generate synthetic examples for rare protocols\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the full fine-tuning pipeline:\n",
    "1. ✅ Data Exploration\n",
    "2. ✅ Data Extraction\n",
    "3. ✅ Dataset Preparation\n",
    "4. ✅ Fine-Tuning\n",
    "5. ✅ Evaluation\n",
    "\n",
    "You now know how to fine-tune language models on blockchain data using QLoRA! 🎉\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Use your fine-tuned model for production inference, or iterate on the training process to improve performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
