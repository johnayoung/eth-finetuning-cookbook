{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52732777",
   "metadata": {},
   "source": [
    "# 03 - Dataset Preparation: Creating Training Data\n",
    "\n",
    "**Goal**: Transform decoded transactions into instruction-tuning format ready for fine-tuning.\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How to extract structured intents from decoded transactions\n",
    "- How to format data in Alpaca instruction-tuning format\n",
    "- How to split data into train/validation/test sets with stratification\n",
    "- How to validate dataset quality\n",
    "- How to visualize dataset characteristics\n",
    "\n",
    "**Prerequisites**: Completed `02-data-extraction.ipynb`, have decoded transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a7c85",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dfd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from eth_finetuning.dataset.intent_extraction import extract_intent, extract_intents_batch\n",
    "from eth_finetuning.dataset.templates import format_training_example, format_training_examples_batch\n",
    "from eth_finetuning.dataset.preparation import prepare_dataset, validate_data\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"✓ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92477bc1",
   "metadata": {},
   "source": [
    "## Loading Decoded Transaction Data\n",
    "\n",
    "First, let's load the decoded transactions we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decoded transactions from previous notebook\n",
    "decoded_file = project_root / \"data\" / \"processed\" / \"decoded_transactions.json\"\n",
    "\n",
    "if decoded_file.exists():\n",
    "    with open(decoded_file, 'r') as f:\n",
    "        decoded_txs = json.load(f)\n",
    "    print(f\"✓ Loaded {len(decoded_txs)} decoded transactions from {decoded_file.name}\")\n",
    "else:\n",
    "    # Fallback: create sample data for demonstration\n",
    "    print(\"⚠️  No decoded transactions file found\")\n",
    "    print(\"   Creating sample data for demonstration...\")\n",
    "    \n",
    "    decoded_txs = [\n",
    "        {\n",
    "            \"tx_hash\": \"0x1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef\",\n",
    "            \"action\": \"transfer\",\n",
    "            \"protocol\": \"ethereum\",\n",
    "            \"from\": \"0x742d35Cc6634C0532925a3b844Bc9e7595f0\",\n",
    "            \"to\": \"0x1f9840a85d5aF5bf1D1762F925BDADdC4201F984\",\n",
    "            \"amount_wei\": \"1000000000000000000\",\n",
    "            \"amount_eth\": \"1.0\",\n",
    "            \"status\": \"success\",\n",
    "        },\n",
    "        {\n",
    "            \"tx_hash\": \"0xabcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890\",\n",
    "            \"action\": \"transfer\",\n",
    "            \"protocol\": \"erc20\",\n",
    "            \"token_address\": \"0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48\",\n",
    "            \"token_symbol\": \"USDC\",\n",
    "            \"from\": \"0x742d35Cc6634C0532925a3b844Bc9e7595f0\",\n",
    "            \"to\": \"0x1f9840a85d5aF5bf1D1762F925BDADdC4201F984\",\n",
    "            \"amount\": \"1000.0\",\n",
    "            \"decimals\": 6,\n",
    "            \"status\": \"success\",\n",
    "        },\n",
    "    ]\n",
    "    print(f\"✓ Created {len(decoded_txs)} sample transactions\")\n",
    "\n",
    "# Display summary\n",
    "protocols = [tx.get('protocol', 'unknown') for tx in decoded_txs]\n",
    "protocol_counts = Counter(protocols)\n",
    "\n",
    "print(\"\\nProtocol Distribution:\")\n",
    "for protocol, count in protocol_counts.most_common():\n",
    "    print(f\"  {protocol:15s}: {count:3d} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3ec91",
   "metadata": {},
   "source": [
    "## Understanding Intent Extraction\n",
    "\n",
    "### What is an Intent?\n",
    "\n",
    "An **intent** is a structured representation of what a transaction does:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"action\": \"transfer\",          // What action: transfer, swap, approve\n",
    "  \"assets\": [\"ETH\"],             // Which assets involved\n",
    "  \"protocol\": \"ethereum\",         // Which protocol\n",
    "  \"outcome\": \"success\",           // Did it succeed?\n",
    "  \"amounts\": [\"1.0\"]              // How much\n",
    "}\n",
    "```\n",
    "\n",
    "### Why Extract Intents?\n",
    "\n",
    "Intents provide a **simplified, consistent** format that:\n",
    "- Makes training data more uniform across protocols\n",
    "- Focuses on semantic meaning, not implementation details\n",
    "- Reduces noise from irrelevant technical fields\n",
    "- Makes it easier for models to learn patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b19a22",
   "metadata": {},
   "source": [
    "## Extracting Intents from Decoded Transactions\n",
    "\n",
    "Let's see how to extract intents from our decoded transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INTENT EXTRACTION EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract intent from first transaction\n",
    "if decoded_txs:\n",
    "    first_tx = decoded_txs[0]\n",
    "    \n",
    "    print(\"\\nInput (Decoded Transaction):\")\n",
    "    print(json.dumps(first_tx, indent=2))\n",
    "    \n",
    "    # Extract intent\n",
    "    intent = extract_intent(first_tx)\n",
    "    \n",
    "    print(\"\\nOutput (Extracted Intent):\")\n",
    "    print(json.dumps(intent, indent=2))\n",
    "    \n",
    "    print(\"\\n✓ Intent extraction successful\")\n",
    "    print(f\"\\nKey observations:\")\n",
    "    print(f\"  • Action simplified to: {intent['action']}\")\n",
    "    print(f\"  • Assets extracted: {intent['assets']}\")\n",
    "    print(f\"  • Amounts normalized: {intent['amounts']}\")\n",
    "    print(f\"  • Outcome captured: {intent['outcome']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e9ea5",
   "metadata": {},
   "source": [
    "### Batch Intent Extraction\n",
    "\n",
    "Now let's extract intents from all our decoded transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35915c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXTRACTING INTENTS FROM ALL TRANSACTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract intents in batch\n",
    "intents = extract_intents_batch(decoded_txs)\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(intents)} intents from {len(decoded_txs)} transactions\")\n",
    "\n",
    "if len(intents) < len(decoded_txs):\n",
    "    failed_count = len(decoded_txs) - len(intents)\n",
    "    print(f\"⚠️  {failed_count} transaction(s) failed intent extraction\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nExample intents:\")\n",
    "for i, intent in enumerate(intents[:3], 1):\n",
    "    print(f\"\\n{i}. Intent:\")\n",
    "    print(f\"   Action:   {intent['action']}\")\n",
    "    print(f\"   Protocol: {intent['protocol']}\")\n",
    "    print(f\"   Assets:   {', '.join(intent['assets'])}\")\n",
    "    print(f\"   Amounts:  {', '.join(intent['amounts'])}\")\n",
    "    print(f\"   Outcome:  {intent['outcome']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da674859",
   "metadata": {},
   "source": [
    "## Instruction-Tuning Format (Alpaca)\n",
    "\n",
    "### What is Instruction-Tuning?\n",
    "\n",
    "Instruction-tuning teaches models to follow instructions through examples of:\n",
    "- **Instruction**: What task to perform\n",
    "- **Input**: The data to process\n",
    "- **Output**: The expected result\n",
    "\n",
    "### Alpaca Format\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Extract the structured intent from this Ethereum transaction.\",\n",
    "  \"input\": \"{...transaction data...}\",\n",
    "  \"output\": \"{...intent JSON...}\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Why This Format?\n",
    "\n",
    "- ✓ Standard format used by many fine-tuning frameworks\n",
    "- ✓ Clear separation of task, input, and expected output\n",
    "- ✓ Compatible with HuggingFace Trainer\n",
    "- ✓ Easy to understand and debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9ef4a",
   "metadata": {},
   "source": [
    "## Formatting Training Examples\n",
    "\n",
    "Let's convert our decoded transactions and intents into training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FORMATTING TRAINING EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Format first transaction as example\n",
    "if decoded_txs and intents:\n",
    "    example = format_training_example(decoded_txs[0], intents[0])\n",
    "    \n",
    "    print(\"\\nTraining Example Structure:\")\n",
    "    print(\"\\n1. INSTRUCTION:\")\n",
    "    print(example['instruction'])\n",
    "    \n",
    "    print(\"\\n2. INPUT (transaction data):\")\n",
    "    input_preview = example['input'][:200] + '...' if len(example['input']) > 200 else example['input']\n",
    "    print(input_preview)\n",
    "    \n",
    "    print(\"\\n3. OUTPUT (intent):\")\n",
    "    output_preview = example['output'][:200] + '...' if len(example['output']) > 200 else example['output']\n",
    "    print(output_preview)\n",
    "    \n",
    "    print(\"\\n✓ Training example formatted successfully\")\n",
    "    print(f\"\\nExample statistics:\")\n",
    "    print(f\"  Instruction length: {len(example['instruction'])} chars\")\n",
    "    print(f\"  Input length:       {len(example['input'])} chars\")\n",
    "    print(f\"  Output length:      {len(example['output'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f029b",
   "metadata": {},
   "source": [
    "### Batch Formatting\n",
    "\n",
    "Now format all transactions into training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5078ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all examples\n",
    "training_examples = format_training_examples_batch(decoded_txs, intents)\n",
    "\n",
    "print(f\"✓ Formatted {len(training_examples)} training examples\")\n",
    "\n",
    "# Analyze example lengths\n",
    "lengths = {\n",
    "    'instruction': [len(ex['instruction']) for ex in training_examples],\n",
    "    'input': [len(ex['input']) for ex in training_examples],\n",
    "    'output': [len(ex['output']) for ex in training_examples],\n",
    "}\n",
    "\n",
    "print(\"\\nLength Statistics (characters):\")\n",
    "for field, values in lengths.items():\n",
    "    print(f\"\\n{field.upper()}:\")\n",
    "    print(f\"  Mean:   {sum(values) / len(values):.1f}\")\n",
    "    print(f\"  Min:    {min(values)}\")\n",
    "    print(f\"  Max:    {max(values)}\")\n",
    "    print(f\"  Median: {sorted(values)[len(values)//2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99950bb5",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "\n",
    "Before splitting, let's validate data quality to ensure:\n",
    "- No missing critical fields\n",
    "- Addresses are properly formatted\n",
    "- Amounts are numeric\n",
    "- Protocols are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfce0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Run validation\n",
    "    validate_data(decoded_txs, intents)\n",
    "    print(\"\\n✓ All validation checks passed\")\n",
    "    \n",
    "    print(\"\\nValidation checks performed:\")\n",
    "    print(\"  ✓ No null values in critical fields\")\n",
    "    print(\"  ✓ Addresses properly checksummed\")\n",
    "    print(\"  ✓ Amounts are numeric\")\n",
    "    print(\"  ✓ Protocols are valid\")\n",
    "    print(\"  ✓ Intents match decoded transactions\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"\\n✗ Validation failed: {e}\")\n",
    "    print(\"\\nPlease fix data quality issues before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129e6b2",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split\n",
    "\n",
    "### Why Split?\n",
    "\n",
    "- **Training set (70%)**: Used to train the model\n",
    "- **Validation set (15%)**: Used to tune hyperparameters and monitor training\n",
    "- **Test set (15%)**: Used for final evaluation (never seen during training)\n",
    "\n",
    "### Stratification\n",
    "\n",
    "We'll use **stratified splitting** to ensure each split has a similar distribution of protocols.\n",
    "This prevents the model from being biased toward certain protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b28b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATASET SPLITTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define output directory\n",
    "output_dir = project_root / \"data\" / \"datasets\"\n",
    "\n",
    "# Prepare dataset with stratified split\n",
    "split_counts = prepare_dataset(\n",
    "    decoded_txs=decoded_txs,\n",
    "    output_dir=output_dir,\n",
    "    split_ratios=(0.7, 0.15, 0.15),\n",
    "    stratify_by_protocol=True,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Dataset prepared and saved\")\n",
    "print(f\"\\nSplit statistics:\")\n",
    "print(f\"  Training:   {split_counts['train']:3d} examples ({split_counts['train']/sum(split_counts.values())*100:.1f}%)\")\n",
    "print(f\"  Validation: {split_counts['validation']:3d} examples ({split_counts['validation']/sum(split_counts.values())*100:.1f}%)\")\n",
    "print(f\"  Test:       {split_counts['test']:3d} examples ({split_counts['test']/sum(split_counts.values())*100:.1f}%)\")\n",
    "print(f\"  Total:      {sum(split_counts.values()):3d} examples\")\n",
    "\n",
    "print(f\"\\nFiles saved to: {output_dir}\")\n",
    "print(f\"  • train.jsonl\")\n",
    "print(f\"  • validation.jsonl\")\n",
    "print(f\"  • test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f196c63",
   "metadata": {},
   "source": [
    "## Visualizing Dataset Characteristics\n",
    "\n",
    "Let's visualize the characteristics of our prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared datasets\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "with open(output_dir / \"train.jsonl\", 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_dir / \"validation.jsonl\", 'r') as f:\n",
    "    val_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(output_dir / \"test.jsonl\", 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"✓ Loaded datasets for visualization\")\n",
    "print(f\"  Train:      {len(train_data)} examples\")\n",
    "print(f\"  Validation: {len(val_data)} examples\")\n",
    "print(f\"  Test:       {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Split sizes\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "counts = [len(train_data), len(val_data), len(test_data)]\n",
    "colors = ['steelblue', 'coral', 'lightgreen']\n",
    "\n",
    "axes[0].bar(splits, counts, color=colors)\n",
    "axes[0].set_title('Dataset Split Sizes', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Examples')\n",
    "axes[0].set_xlabel('Split')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (split, count) in enumerate(zip(splits, counts)):\n",
    "    axes[0].text(i, count + max(counts)*0.02, str(count), \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Length distribution\n",
    "all_lengths = {\n",
    "    'Train': [len(ex['input']) + len(ex['output']) for ex in train_data],\n",
    "    'Val': [len(ex['input']) + len(ex['output']) for ex in val_data],\n",
    "    'Test': [len(ex['input']) + len(ex['output']) for ex in test_data],\n",
    "}\n",
    "\n",
    "axes[1].boxplot([all_lengths['Train'], all_lengths['Val'], all_lengths['Test']], \n",
    "                labels=splits)\n",
    "axes[1].set_title('Example Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Characters (Input + Output)')\n",
    "axes[1].set_xlabel('Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80651778",
   "metadata": {},
   "source": [
    "## Protocol Distribution Across Splits\n",
    "\n",
    "Let's verify that stratification worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract protocols from outputs (intents)\n",
    "def get_protocols(data):\n",
    "    protocols = []\n",
    "    for example in data:\n",
    "        try:\n",
    "            intent = json.loads(example['output'])\n",
    "            protocols.append(intent.get('protocol', 'unknown'))\n",
    "        except:\n",
    "            protocols.append('unknown')\n",
    "    return Counter(protocols)\n",
    "\n",
    "train_protocols = get_protocols(train_data)\n",
    "val_protocols = get_protocols(val_data)\n",
    "test_protocols = get_protocols(test_data)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "protocol_names = set(train_protocols.keys()) | set(val_protocols.keys()) | set(test_protocols.keys())\n",
    "comparison = []\n",
    "\n",
    "for protocol in protocol_names:\n",
    "    comparison.append({\n",
    "        'Protocol': protocol,\n",
    "        'Train': train_protocols.get(protocol, 0),\n",
    "        'Validation': val_protocols.get(protocol, 0),\n",
    "        'Test': test_protocols.get(protocol, 0),\n",
    "    })\n",
    "\n",
    "df_protocols = pd.DataFrame(comparison)\n",
    "\n",
    "# Calculate percentages\n",
    "df_protocols['Train %'] = (df_protocols['Train'] / df_protocols['Train'].sum() * 100).round(1)\n",
    "df_protocols['Val %'] = (df_protocols['Validation'] / df_protocols['Validation'].sum() * 100).round(1)\n",
    "df_protocols['Test %'] = (df_protocols['Test'] / df_protocols['Test'].sum() * 100).round(1)\n",
    "\n",
    "print(\"PROTOCOL DISTRIBUTION ACROSS SPLITS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_protocols.to_string(index=False))\n",
    "\n",
    "# Check if distributions are similar\n",
    "print(\"\\n✓ Stratification ensures balanced protocol representation across splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e619e7",
   "metadata": {},
   "source": [
    "## Loading Dataset with HuggingFace\n",
    "\n",
    "Let's verify that our dataset can be loaded by HuggingFace's `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d060591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"LOADING DATASET WITH HUGGINGFACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\n",
    "        'json',\n",
    "        data_files={\n",
    "            'train': str(output_dir / 'train.jsonl'),\n",
    "            'validation': str(output_dir / 'validation.jsonl'),\n",
    "            'test': str(output_dir / 'test.jsonl'),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Dataset loaded successfully with HuggingFace\")\n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample from training set:\")\n",
    "    sample = dataset['train'][0]\n",
    "    print(f\"\\nInstruction: {sample['instruction'][:100]}...\")\n",
    "    print(f\"Input:       {sample['input'][:100]}...\")\n",
    "    print(f\"Output:      {sample['output'][:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149af591",
   "metadata": {},
   "source": [
    "## Quality Checks\n",
    "\n",
    "Let's perform final quality checks on our prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATASET QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_data = train_data + val_data + test_data\n",
    "\n",
    "# Check 1: All examples have required fields\n",
    "required_fields = ['instruction', 'input', 'output']\n",
    "missing_fields = 0\n",
    "for example in all_data:\n",
    "    for field in required_fields:\n",
    "        if field not in example or not example[field]:\n",
    "            missing_fields += 1\n",
    "\n",
    "print(f\"\\n✓ Check 1: Required fields\")\n",
    "if missing_fields == 0:\n",
    "    print(f\"  All {len(all_data)} examples have all required fields\")\n",
    "else:\n",
    "    print(f\"  ⚠️  {missing_fields} missing field(s) detected\")\n",
    "\n",
    "# Check 2: Output is valid JSON\n",
    "invalid_json = 0\n",
    "for example in all_data:\n",
    "    try:\n",
    "        json.loads(example['output'])\n",
    "    except json.JSONDecodeError:\n",
    "        invalid_json += 1\n",
    "\n",
    "print(f\"\\n✓ Check 2: Valid JSON outputs\")\n",
    "if invalid_json == 0:\n",
    "    print(f\"  All {len(all_data)} outputs are valid JSON\")\n",
    "else:\n",
    "    print(f\"  ⚠️  {invalid_json} invalid JSON output(s) detected\")\n",
    "\n",
    "# Check 3: Reasonable length distribution\n",
    "total_lengths = [len(ex['input']) + len(ex['output']) for ex in all_data]\n",
    "avg_length = sum(total_lengths) / len(total_lengths)\n",
    "max_length = max(total_lengths)\n",
    "\n",
    "print(f\"\\n✓ Check 3: Length distribution\")\n",
    "print(f\"  Average total length: {avg_length:.0f} characters\")\n",
    "print(f\"  Maximum total length: {max_length} characters\")\n",
    "if max_length < 2048 * 4:  # Rough char to token ratio\n",
    "    print(f\"  ✓ All examples within reasonable token limits\")\n",
    "else:\n",
    "    print(f\"  ⚠️  Some examples may exceed token limits\")\n",
    "\n",
    "# Check 4: No duplicates\n",
    "unique_inputs = set(ex['input'] for ex in all_data)\n",
    "duplicate_count = len(all_data) - len(unique_inputs)\n",
    "\n",
    "print(f\"\\n✓ Check 4: Duplicates\")\n",
    "if duplicate_count == 0:\n",
    "    print(f\"  No duplicate examples detected\")\n",
    "else:\n",
    "    print(f\"  ⚠️  {duplicate_count} duplicate example(s) detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Quality checks complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139c53d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✓ **Intent Extraction**: Simplifies complex transaction data into consistent semantic structure\n",
    "\n",
    "✓ **Instruction Format**: Alpaca format clearly separates task, input, and expected output\n",
    "\n",
    "✓ **Stratified Splitting**: Ensures balanced protocol distribution across train/val/test\n",
    "\n",
    "✓ **Data Validation**: Critical for catching quality issues before training\n",
    "\n",
    "✓ **JSONL Format**: One example per line, compatible with HuggingFace\n",
    "\n",
    "## Troubleshooting Tips\n",
    "\n",
    "**Issue**: Validation fails with missing fields\n",
    "- **Solution**: Check decoded transaction structure, ensure all decoders return required fields\n",
    "\n",
    "**Issue**: Imbalanced splits (one protocol dominates)\n",
    "- **Solution**: Use stratified splitting, or collect more data from underrepresented protocols\n",
    "\n",
    "**Issue**: Examples too long (exceed token limits)\n",
    "- **Solution**: Truncate input data, focus on essential fields only\n",
    "\n",
    "**Issue**: Invalid JSON in outputs\n",
    "- **Solution**: Check intent extraction logic, ensure proper JSON serialization\n",
    "\n",
    "**Issue**: HuggingFace dataset loading fails\n",
    "- **Solution**: Verify JSONL format (one JSON object per line), check file paths\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**04-fine-tuning.ipynb**), we'll learn how to:\n",
    "- Configure QLoRA for efficient fine-tuning\n",
    "- Load and prepare the base model\n",
    "- Execute training with live monitoring\n",
    "- Manage VRAM usage on consumer GPUs\n",
    "- Save and checkpoint models\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** → `notebooks/04-fine-tuning.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
