"""
Report generation for model evaluation results.

This module provides functions for creating human-readable markdown reports
from evaluation metrics, including tables, confusion matrices, and per-protocol
performance breakdowns.

Usage:
    from eth_finetuning.evaluation.report import generate_markdown_report

    generate_markdown_report(metrics, predictions, ground_truth, "report.md")
"""

import logging
from datetime import datetime
from pathlib import Path
from typing import Any

import numpy as np

logger = logging.getLogger(__name__)


def generate_markdown_report(
    metrics: dict[str, Any],
    predictions: list[dict[str, Any]],
    ground_truth: list[dict[str, Any]],
    output_path: str | Path,
) -> None:
    """
    Generate comprehensive markdown evaluation report.

    Creates a human-readable report with:
    - Summary metrics table
    - Per-protocol performance breakdown
    - Confusion matrix
    - Sample predictions (correct and incorrect)
    - Recommendations for improvement

    Args:
        metrics: Dictionary of calculated metrics from calculate_accuracy_metrics
        predictions: List of predicted intent dictionaries
        ground_truth: List of ground truth intent dictionaries
        output_path: Path to save markdown report

    Notes:
        - Report includes timestamp and model information
        - Tables formatted for easy reading
        - Includes pass/fail indicators against targets
        - Provides actionable recommendations
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    logger.info(f"Generating markdown report: {output_path}")

    with open(output_path, "w") as f:
        # Header
        f.write("# Ethereum Intent Extraction - Evaluation Report\n\n")
        f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        # Summary Section
        f.write("## Summary Metrics\n\n")
        f.write(_format_summary_table(metrics))
        f.write("\n\n")

        # Success Criteria Check
        f.write("## Success Criteria\n\n")
        f.write(_format_success_criteria(metrics))
        f.write("\n\n")

        # Per-Protocol Breakdown
        if "per_protocol_metrics" in metrics and metrics["per_protocol_metrics"]:
            f.write("## Per-Protocol Performance\n\n")
            f.write(_format_per_protocol_table(metrics["per_protocol_metrics"]))
            f.write("\n\n")

        # Confusion Matrix
        if "confusion_matrix" in metrics and "confusion_matrix_labels" in metrics:
            f.write("## Confusion Matrix\n\n")
            f.write(
                _format_confusion_matrix(
                    metrics["confusion_matrix"],
                    metrics["confusion_matrix_labels"],
                )
            )
            f.write("\n\n")

        # Sample Predictions
        f.write("## Sample Predictions\n\n")
        f.write(_format_sample_predictions(predictions, ground_truth))
        f.write("\n\n")

        # Recommendations
        f.write("## Recommendations\n\n")
        f.write(_format_recommendations(metrics))
        f.write("\n\n")

        # Footer
        f.write("---\n\n")
        f.write("*Report generated by eth-finetuning-cookbook evaluation module*\n")

    logger.info(f"Report saved to {output_path}")


def _format_summary_table(metrics: dict[str, Any]) -> str:
    """Format summary metrics as markdown table."""
    lines = []
    lines.append("| Metric | Value | Target | Status |")
    lines.append("|--------|-------|--------|--------|")

    # Overall Accuracy
    overall = metrics.get("overall_accuracy", 0.0)
    status = "✅ PASS" if overall >= 0.90 else "❌ FAIL"
    lines.append(f"| Overall Accuracy | {overall:.2%} | ≥90% | {status} |")

    # Amount Accuracy
    amount = metrics.get("amount_accuracy", 0.0)
    status = "✅ PASS" if amount >= 0.90 else "❌ FAIL"
    lines.append(f"| Amount Accuracy | {amount:.2%} | ≥90% | {status} |")

    # Address Accuracy
    address = metrics.get("address_accuracy", 0.0)
    status = "✅ PASS" if address >= 0.90 else "❌ FAIL"
    lines.append(f"| Address Accuracy | {address:.2%} | ≥90% | {status} |")

    # Protocol Accuracy
    protocol = metrics.get("protocol_accuracy", 0.0)
    status = "✅ PASS" if protocol >= 0.90 else "❌ FAIL"
    lines.append(f"| Protocol Accuracy | {protocol:.2%} | ≥90% | {status} |")

    # Flesch Score (if available)
    flesch = metrics.get("flesch_score", 0.0)
    if flesch > 0:
        status = "✅ PASS" if flesch >= 60 else "❌ FAIL"
        lines.append(f"| Flesch Reading Ease | {flesch:.1f} | ≥60 | {status} |")

    # Sample counts
    total = metrics.get("total_samples", 0)
    failed = metrics.get("failed_parses", 0)
    lines.append(f"| Total Samples | {total} | - | - |")
    lines.append(f"| Failed Parses | {failed} | 0 | {'✅' if failed == 0 else '⚠️'} |")

    return "\n".join(lines)


def _format_success_criteria(metrics: dict[str, Any]) -> str:
    """Format success criteria checklist."""
    lines = []

    overall = metrics.get("overall_accuracy", 0.0)
    amount = metrics.get("amount_accuracy", 0.0)
    address = metrics.get("address_accuracy", 0.0)
    protocol = metrics.get("protocol_accuracy", 0.0)
    flesch = metrics.get("flesch_score", 0.0)
    failed = metrics.get("failed_parses", 0)

    lines.append("Target requirements from ROADMAP.md:\n")

    # Accuracy targets
    if overall >= 0.90:
        lines.append("- ✅ Model achieves ≥90% overall accuracy")
    else:
        lines.append(
            f"- ❌ Model achieves ≥90% overall accuracy (current: {overall:.2%})"
        )

    if amount >= 0.90 and address >= 0.90 and protocol >= 0.90:
        lines.append(
            "- ✅ Model achieves ≥90% accuracy on amounts, addresses, and protocols"
        )
    else:
        lines.append(
            "- ❌ Model achieves ≥90% accuracy on amounts, addresses, and protocols"
        )

    # Readability target
    if flesch > 0:
        if flesch >= 60:
            lines.append("- ✅ Flesch Reading Ease ≥60 for generated text")
        else:
            lines.append(
                f"- ❌ Flesch Reading Ease ≥60 for generated text (current: {flesch:.1f})"
            )
    else:
        lines.append(
            "- ⚠️  Flesch Reading Ease not calculated (no text descriptions generated)"
        )

    # Completion check
    if failed == 0:
        lines.append("- ✅ Evaluation completes on full test set without errors")
    else:
        lines.append(f"- ⚠️  Evaluation completed with {failed} failed parses")

    # Overall status
    lines.append("\n**Overall Status:** ")
    if overall >= 0.90 and failed == 0:
        lines.append("🎉 **PASSING** - All success criteria met!")
    elif overall >= 0.85:
        lines.append("⚠️  **CLOSE** - Near target, minor improvements needed")
    else:
        lines.append("❌ **NEEDS WORK** - Significant improvements required")

    return "\n".join(lines)


def _format_per_protocol_table(per_protocol: dict[str, dict[str, float]]) -> str:
    """Format per-protocol metrics as markdown table."""
    lines = []
    lines.append("| Protocol | Samples | Overall | Amount | Address | Protocol |")
    lines.append("|----------|---------|---------|--------|---------|----------|")

    for protocol, metrics in sorted(per_protocol.items()):
        samples = metrics.get("total_samples", 0)
        overall = metrics.get("overall_accuracy", 0.0)
        amount = metrics.get("amount_accuracy", 0.0)
        address = metrics.get("address_accuracy", 0.0)
        proto_acc = metrics.get("protocol_accuracy", 0.0)

        lines.append(
            f"| {protocol} | {samples} | {overall:.2%} | {amount:.2%} | "
            f"{address:.2%} | {proto_acc:.2%} |"
        )

    lines.append("\n**Insights:**")

    # Find best/worst protocols
    if per_protocol:
        sorted_protocols = sorted(
            per_protocol.items(),
            key=lambda x: x[1].get("overall_accuracy", 0.0),
            reverse=True,
        )

        best = sorted_protocols[0]
        worst = sorted_protocols[-1]

        lines.append(
            f"- Best performing: **{best[0]}** ({best[1]['overall_accuracy']:.2%})"
        )
        lines.append(
            f"- Needs improvement: **{worst[0]}** ({worst[1]['overall_accuracy']:.2%})"
        )

    return "\n".join(lines)


def _format_confusion_matrix(
    cm: list[list[int]],
    labels: list[str],
) -> str:
    """Format confusion matrix as markdown table."""
    lines = []

    lines.append("**Protocol Classification Confusion Matrix**\n")
    lines.append("(Rows = Ground Truth, Columns = Predicted)\n")

    # Header row
    header = "| True \\ Pred |" + "|".join(f" {label} " for label in labels) + "|"
    lines.append(header)

    # Separator
    separator = "|" + "|".join("---" for _ in range(len(labels) + 1)) + "|"
    lines.append(separator)

    # Data rows
    cm_array = np.array(cm)
    for i, label in enumerate(labels):
        row = f"| **{label}** |"
        for j in range(len(labels)):
            count = cm_array[i, j]
            # Highlight diagonal (correct predictions)
            if i == j:
                row += f" **{count}** |"
            else:
                row += f" {count} |"
        lines.append(row)

    # Calculate per-class recall
    lines.append("\n**Per-Class Recall:**")
    for i, label in enumerate(labels):
        total = cm_array[i].sum()
        correct = cm_array[i, i]
        recall = correct / total if total > 0 else 0.0
        lines.append(f"- {label}: {recall:.2%} ({correct}/{total})")

    return "\n".join(lines)


def _format_sample_predictions(
    predictions: list[dict[str, Any]],
    ground_truth: list[dict[str, Any]],
    num_correct: int = 3,
    num_incorrect: int = 3,
) -> str:
    """Format sample predictions (correct and incorrect)."""
    lines = []

    # Find correct and incorrect predictions
    correct_samples = []
    incorrect_samples = []

    for i, (pred, truth) in enumerate(zip(predictions, ground_truth)):
        if not pred or not truth:
            continue

        # Simple comparison: protocols match
        pred_proto = pred.get("protocol", "").lower()
        truth_proto = truth.get("protocol", "").lower()

        if pred_proto == truth_proto:
            correct_samples.append((i, pred, truth))
        else:
            incorrect_samples.append((i, pred, truth))

    # Show correct examples
    lines.append("### Correct Predictions (Sample)\n")
    for i, pred, truth in correct_samples[:num_correct]:
        lines.append(f"**Example {i+1}** ✅")
        lines.append("```json")
        lines.append("Predicted: " + str(pred))
        lines.append("```\n")

    if not correct_samples:
        lines.append("*No correct predictions found*\n")

    # Show incorrect examples
    lines.append("### Incorrect Predictions (Sample)\n")
    for i, pred, truth in incorrect_samples[:num_incorrect]:
        lines.append(f"**Example {i+1}** ❌")
        lines.append("```json")
        lines.append("Predicted: " + str(pred))
        lines.append("Ground Truth: " + str(truth))
        lines.append("```\n")

    if not incorrect_samples:
        lines.append("*No incorrect predictions found*\n")

    return "\n".join(lines)


def _format_recommendations(metrics: dict[str, Any]) -> str:
    """Generate recommendations based on metrics."""
    lines = []

    overall = metrics.get("overall_accuracy", 0.0)
    amount = metrics.get("amount_accuracy", 0.0)
    address = metrics.get("address_accuracy", 0.0)
    protocol = metrics.get("protocol_accuracy", 0.0)
    failed = metrics.get("failed_parses", 0)

    lines.append("Based on evaluation results:\n")

    # Overall performance
    if overall >= 0.90:
        lines.append("✅ **Model meets accuracy targets!** Consider:")
        lines.append("   - Deploying for production use")
        lines.append("   - Testing on additional datasets")
        lines.append("   - Monitoring real-world performance")
    elif overall >= 0.80:
        lines.append("⚠️  **Model performance is close to target.** Suggestions:")
        lines.append("   - Increase training epochs or data size")
        lines.append("   - Fine-tune hyperparameters (learning rate, LoRA rank)")
        lines.append("   - Add more examples for underperforming protocols")
    else:
        lines.append("❌ **Model needs significant improvement.** Recommendations:")
        lines.append("   - Review training data quality and diversity")
        lines.append("   - Increase training duration (more epochs)")
        lines.append("   - Consider larger base model or higher LoRA rank")
        lines.append("   - Check for data leakage or train/test contamination")

    # Specific issues
    if amount < 0.80:
        lines.append("\n⚠️  **Amount Accuracy Low:**")
        lines.append("   - Review numeric formatting in training data")
        lines.append("   - Ensure consistent units (Wei vs Ether)")
        lines.append("   - Add more examples with various amount formats")

    if address < 0.80:
        lines.append("\n⚠️  **Address Accuracy Low:**")
        lines.append("   - Verify address checksumming in training data")
        lines.append("   - Ensure model learns to differentiate 'from' vs 'to'")
        lines.append("   - Add examples with similar but different addresses")

    if protocol < 0.80:
        lines.append("\n⚠️  **Protocol Accuracy Low:**")
        lines.append("   - Balance protocol distribution in training data")
        lines.append("   - Add more distinguishing features for each protocol")
        lines.append("   - Review per-protocol metrics for specific issues")

    if failed > 0:
        lines.append(f"\n⚠️  **{failed} Failed Parses:**")
        lines.append("   - Review model outputs for JSON formatting issues")
        lines.append("   - Add more examples with correct JSON structure")
        lines.append("   - Consider post-processing to fix common formatting errors")

    return "\n".join(lines)
