# Training Configuration for Ethereum Intent Extraction Fine-Tuning
# Optimized for 12GB VRAM GPUs (e.g., RTX 3060)

# Base Model Configuration
model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"  # Base model for fine-tuning
  cache_dir: "models/base"  # Directory to cache downloaded models
  trust_remote_code: false  # Security setting for custom model code

# QLoRA Configuration (4-bit Quantization)
quantization:
  load_in_4bit: true  # Enable 4-bit quantization (reduces VRAM from ~28GB to ~7GB)
  bnb_4bit_compute_dtype: "float16"  # Computation dtype for quantized layers
  bnb_4bit_quant_type: "nf4"  # Quantization type (NormalFloat4 is optimal for QLoRA)
  bnb_4bit_use_double_quant: true  # Additional quantization for constants (saves ~0.5GB)

# LoRA Configuration (Parameter-Efficient Fine-Tuning)
lora:
  r: 16  # LoRA rank - balance between expressiveness and memory (8-64 typical range)
  lora_alpha: 32  # LoRA scaling factor (typically 2x rank)
  lora_dropout: 0.05  # Dropout for LoRA layers (0.05-0.1 prevents overfitting)
  bias: "none"  # Bias parameter handling ("none", "all", or "lora_only")
  task_type: "CAUSAL_LM"  # Task type for PEFT
  target_modules:  # Attention modules to apply LoRA to
    - "q_proj"  # Query projection
    - "v_proj"  # Value projection
    # Note: Can also add "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"
    # for more parameters but higher VRAM usage

# Training Hyperparameters
training:
  # Learning rate
  learning_rate: 0.0002  # 2e-4 is standard for QLoRA fine-tuning
  
  # Batch configuration (critical for memory constraints)
  per_device_train_batch_size: 1  # Samples per GPU per forward pass
  per_device_eval_batch_size: 1  # Evaluation batch size
  gradient_accumulation_steps: 16  # Effective batch size = batch_size * accumulation
  
  # Training duration
  num_train_epochs: 3  # Number of complete passes through training data
  max_steps: -1  # -1 means use num_train_epochs instead
  
  # Sequence length
  max_seq_length: 2048  # Maximum token length for inputs/outputs
  
  # Optimization
  optim: "paged_adamw_32bit"  # Optimizer (paged_adamw for memory efficiency)
  weight_decay: 0.01  # L2 regularization
  warmup_steps: 100  # Linear warmup for learning rate
  lr_scheduler_type: "cosine"  # Learning rate schedule
  
  # Mixed precision training
  fp16: false  # Use FP16 if supported (automatic fallback to BF16 on Ampere+)
  bf16: true  # Use BF16 on Ampere+ GPUs (RTX 30XX and newer)
  
  # Gradient optimization
  gradient_checkpointing: true  # Trade compute for memory (saves ~30% VRAM)
  max_grad_norm: 1.0  # Gradient clipping threshold
  
  # Logging
  logging_steps: 10  # Log metrics every N steps
  logging_first_step: true  # Log first step for debugging
  
  # Evaluation
  evaluation_strategy: "steps"  # "steps" or "epoch"
  eval_steps: 500  # Evaluate every N steps
  
  # Checkpointing
  save_strategy: "steps"  # "steps" or "epoch"
  save_steps: 500  # Save checkpoint every N steps
  save_total_limit: 3  # Keep only last N checkpoints (saves disk space)
  
  # Output
  output_dir: "models/fine-tuned/eth-intent-extractor"  # Directory for model outputs
  overwrite_output_dir: false  # Overwrite existing output directory
  
  # Reproducibility
  seed: 42  # Random seed for reproducibility
  
  # Performance
  dataloader_num_workers: 4  # Number of workers for data loading
  dataloader_pin_memory: true  # Pin memory for faster GPU transfer
  
  # Reporting
  report_to: []  # Integrations: ["tensorboard", "wandb", "mlflow"] (empty for none)
  
  # Checkpoint recovery
  resume_from_checkpoint: null  # Path to checkpoint to resume from (null = start fresh)

# Dataset Configuration
dataset:
  train_file: "data/datasets/train.jsonl"  # Path to training data
  validation_file: "data/datasets/validation.jsonl"  # Path to validation data
  test_file: "data/datasets/test.jsonl"  # Path to test data
  
  # Data preprocessing
  max_train_samples: null  # Limit training samples (null = use all)
  max_eval_samples: null  # Limit evaluation samples (null = use all)
  
  # Streaming mode (for very large datasets that don't fit in memory)
  streaming: false
  
  # Data processing
  preprocessing_num_workers: 4  # Number of workers for dataset preprocessing
  
  # Token padding
  padding: "max_length"  # "max_length" or "longest"

# Prompt Template
prompt:
  # Template for instruction-following format
  template: |
    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
    
    ### Instruction:
    {instruction}
    
    ### Input:
    {input}
    
    ### Response:
    {output}
  
  # Special tokens
  instruction_key: "instruction"
  input_key: "input"
  output_key: "output"

# Monitoring & Callbacks
monitoring:
  # Early stopping (optional)
  early_stopping:
    enabled: false  # Enable early stopping based on validation loss
    patience: 3  # Number of evaluations with no improvement before stopping
    threshold: 0.01  # Minimum change to qualify as improvement
  
  # Model checkpointing strategy
  checkpoint:
    save_best_only: false  # Only save checkpoint with best eval loss
    metric: "eval_loss"  # Metric to monitor for best checkpoint
